{
 "cells": [
  {
   "metadata": {},
   "id": "0ff412e6",
   "cell_type": "markdown",
   "source": "# Machine Learning Interview Questions and Answers\n\nThis notebook contains a series of commonly asked machine learning interview questions along with detailed answers and code implementations. It is designed to help you prepare for technical interviews related to machine learning and data science.\n\n## Table of Contents\n1. **Conceptual Questions**\n    - What is the difference between bias and variance?\n    - Explain how gradient descent works.\n    - What are the common types of neural network architectures?\n2. **Programming Challenges**\n    - Implement logistic regression from scratch.\n    - Build a decision tree classifier using scikit-learn.\n    - Create a neural network model to classify the MNIST dataset using TensorFlow.\n3. **Advanced Questions**\n    - How would you implement a custom loss function in TensorFlow?\n    - Describe how reinforcement learning differs from supervised learning.\n"
  },
  {
   "metadata": {},
   "id": "6e034957",
   "cell_type": "markdown",
   "source": "## 1. Conceptual Questions\n\n### 1.1 What is the difference between bias and variance?\n- **Bias** is the error introduced by approximating a real-world problem, which may be complex, by a simpler model. High bias can cause underfitting.\n- **Variance** is the error introduced by the model's sensitivity to small fluctuations in the training data. High variance can cause overfitting.\n\n**Answer**: The trade-off between bias and variance is crucial when building machine learning models. Ideally, we aim for a model with low bias and low variance, but this is often challenging to achieve. Techniques such as cross-validation and regularization are commonly used to address this trade-off.\n"
  },
  {
   "metadata": {},
   "id": "b83ad306",
   "cell_type": "markdown",
   "source": "### 1.2 Explain how gradient descent works.\n**Gradient Descent** is an optimization algorithm used to minimize a function by iteratively moving towards the steepest descent, defined by the negative of the gradient. It is commonly used in machine learning to update the weights of models such as linear regression and neural networks.\n\n- **Step 1**: Initialize the weights randomly.\n- **Step 2**: Calculate the gradient of the loss function with respect to the weights.\n- **Step 3**: Update the weights using the gradient and a learning rate: `weights = weights - learning_rate * gradient`.\n- **Step 4**: Repeat until convergence.\n\n**Answer**: Different variations of gradient descent exist, such as Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent, and Batch Gradient Descent. The choice depends on the dataset size and convergence requirements.\n"
  },
  {
   "metadata": {},
   "id": "e1c92dbe",
   "cell_type": "markdown",
   "source": "## 2. Programming Challenges\n\n### 2.1 Implement Logistic Regression from Scratch\nImplement a logistic regression model using only NumPy. This exercise tests your understanding of the mathematics behind logistic regression and your ability to implement algorithms from scratch.\n\n#### Problem Statement\n- Implement a binary logistic regression model using NumPy.\n- Train it on a small dataset and report accuracy.\n\n#### Solution\nThe logistic regression model is defined as:\n$$ y = \\sigma(X \\cdot W + b) $$\nWhere:\n- $\\sigma$ is the sigmoid function.\n- $X$ is the input matrix.\n- $W$ is the weight vector.\n- $b$ is the bias term.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "b48efb81",
   "cell_type": "code",
   "source": "import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Sigmoid function\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# Logistic Regression implementation\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, epochs=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        # Initialize weights and bias\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.epochs):\n            linear_model = np.dot(X, self.weights) + self.bias\n            y_predicted = sigmoid(linear_model)\n\n            # Compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n\n            # Update weights\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights) + self.bias\n        y_predicted = sigmoid(linear_model)\n        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n        return np.array(y_predicted_cls)\n\n# Generate a dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train logistic regression model\nmodel = LogisticRegression(learning_rate=0.01, epochs=1000)\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f'Logistic Regression Model Accuracy: {accuracy:.2f}')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "id": "c000bba6",
   "cell_type": "markdown",
   "source": "### 2.2 Build a Decision Tree Classifier using scikit-learn\nCreate a decision tree classifier using scikit-learn on the famous Iris dataset.\n\n#### Problem Statement\n- Load the Iris dataset.\n- Split the data into training and testing sets.\n- Train a decision tree classifier and report the accuracy.\n\n#### Solution\nWe use the `DecisionTreeClassifier` from `sklearn.tree` to build the model.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "1a977f14",
   "cell_type": "code",
   "source": "from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the Decision Tree model\ndt_classifier = DecisionTreeClassifier(random_state=42)\ndt_classifier.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = dt_classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Decision Tree Classifier Accuracy: {accuracy:.2f}')\n",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}