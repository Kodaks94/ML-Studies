{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729c6c5ad105da2e",
   "metadata": {},
   "source": [
    "# Machine Learning Revision Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d59d4e68212f65",
   "metadata": {},
   "source": [
    "## 1. Basic Machine Learning Concepts\n",
    "\n",
    "### 1.1 Supervised vs. Unsupervised Learning\n",
    "\n",
    "#### Supervised Learning\n",
    "Supervised learning refers to a class of algorithms that learn from *labeled data*, where each input is paired with an output label. The goal is to learn a function that maps inputs to outputs based on example input-output pairs. The algorithm tries to minimize the error between its predictions and the actual labels.\n",
    "\n",
    "- **Examples**:\n",
    "  - **Regression**: Predicting continuous values (e.g., house prices, stock prices).\n",
    "  - **Classification**: Predicting discrete values (e.g., spam detection, image classification).\n",
    "\n",
    "- **Key characteristics**:\n",
    "  - Requires labeled data.\n",
    "  - Has clear goals defined by the labels.\n",
    "  - Can be used for both classification and regression tasks.\n",
    "\n",
    "- **Common Algorithms**:\n",
    "  - Linear Regression\n",
    "  - Logistic Regression\n",
    "  - Support Vector Machines (SVM)\n",
    "  - Decision Trees\n",
    "  - Neural Networks\n",
    "\n",
    "#### Unsupervised Learning\n",
    "Unsupervised learning deals with *unlabeled data*. The goal is to identify hidden patterns or intrinsic structures within the data without predefined labels.\n",
    "\n",
    "- **Examples**:\n",
    "  - **Clustering**: Grouping similar items together (e.g., customer segmentation).\n",
    "  - **Association Rule Learning**: Discovering relationships between variables in large datasets (e.g., market basket analysis).\n",
    "\n",
    "- **Key characteristics**:\n",
    "  - No labeled output.\n",
    "  - Learns the underlying structure from the data itself.\n",
    "  - Often used for exploratory data analysis.\n",
    "\n",
    "- **Common Algorithms**:\n",
    "  - K-Means Clustering\n",
    "  - Hierarchical Clustering\n",
    "  - Principal Component Analysis (PCA)\n",
    "  - t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "  - Association Rule Mining (e.g., Apriori algorithm)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Common Machine Learning Algorithms\n",
    "\n",
    "#### Linear Regression\n",
    "Linear regression is a simple, interpretable model used for predicting continuous outcomes based on linear relationships between the input features and the target variable. The model assumes a linear dependency, represented as:\n",
    "\n",
    "$$\\[\n",
    "y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon\n",
    "\\]$$\n",
    "\n",
    "where:\n",
    "- $$\\( y \\)$$ is the target variable.\n",
    "- $\\( x_1, x_2, \\ldots, x_n \\)$ are input features.\n",
    "- $\\( \\beta_0 \\)$ is the intercept.\n",
    "- $\\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\)$ are the coefficients.\n",
    "- $\\( \\epsilon \\)$ is the error term.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple and interpretable.\n",
    "  - Computationally efficient.\n",
    "  - Works well when there is a linear relationship between input features and the target.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Poor performance when there are complex nonlinear relationships.\n",
    "  - Sensitive to outliers.\n",
    "\n",
    "#### Logistic Regression\n",
    "Logistic regression is used for binary classification tasks. It estimates the probability of a binary outcome by using the logistic (sigmoid) function to map predicted values to probabilities:\n",
    "\n",
    "$\\[\n",
    "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n)}}\n",
    "\\]$\n",
    "\n",
    "- **Advantages**:\n",
    "  - Good for binary classification with linear boundaries.\n",
    "  - Can handle unbalanced datasets using techniques like class weights.\n",
    "  \n",
    "- **Limitations**:\n",
    "  - Not suitable for complex relationships.\n",
    "  - Can overfit with high-dimensional data if regularization is not used.\n",
    "\n",
    "#### Decision Trees and Random Forests\n",
    "- **Decision Trees**:\n",
    "  - A tree-based model that splits the data recursively based on feature values, creating branches that lead to decisions or predictions at the leaves.\n",
    "  - The splits are based on criteria like Information Gain, Gini Impurity, or Chi-Square statistics.\n",
    "\n",
    "- **Random Forests**:\n",
    "  - An ensemble method that builds multiple decision trees and combines their results to improve prediction accuracy and control overfitting.\n",
    "  - It reduces variance by averaging multiple trees, each trained on a different subset of the data.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Can handle both numerical and categorical features.\n",
    "  - Less preprocessing required (e.g., no need for feature scaling).\n",
    "\n",
    "- **Limitations**:\n",
    "  - Can be prone to overfitting (especially decision trees).\n",
    "  - High computational cost for large forests.\n",
    "\n",
    "#### K-Means Clustering\n",
    "K-means clustering is an unsupervised algorithm that partitions data into $\\( K \\)$ clusters based on feature similarity. The algorithm works by:\n",
    "\n",
    "1. Initializing $\\( K \\)$ cluster centroids.\n",
    "2. Assigning each data point to the nearest centroid.\n",
    "3. Updating centroids by calculating the mean of assigned points.\n",
    "4. Repeating the process until convergence.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple and easy to implement.\n",
    "  - Scales well with a large number of samples.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Requires pre-specifying the number of clusters $\\( K \\)$.\n",
    "  - Sensitive to outliers and initial centroid selection.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Evaluation Metrics\n",
    "\n",
    "#### Classification Metrics\n",
    "Evaluating classification models involves assessing how well they distinguish between classes. Common metrics include:\n",
    "\n",
    "- **Accuracy**: Proportion of correctly predicted labels out of the total predictions.\n",
    "\n",
    "$\\[\n",
    "\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Samples}}\n",
    "\\]$\n",
    "\n",
    "- **Precision**: Proportion of correctly predicted positive observations out of all predicted positives.\n",
    "\n",
    "$\\[\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "\\]$\n",
    "\n",
    "- **Recall**: Proportion of correctly predicted positive observations out of all actual positives.\n",
    "\n",
    "$\\[\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "\\]$\n",
    "\n",
    "- **F1-Score**: Harmonic mean of precision and recall. It balances the two when dealing with unbalanced datasets.\n",
    "\n",
    "$\\[\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]$\n",
    "\n",
    "- **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**: Measures the trade-off between true positive rate and false positive rate across different threshold values.\n",
    "\n",
    "#### Regression Metrics\n",
    "Regression models predict continuous values, so their performance is evaluated using metrics that assess prediction error:\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Average squared difference between actual and predicted values. Sensitive to outliers.\n",
    "\n",
    "$\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "\\]$\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: Average absolute difference between actual and predicted values. Provides a more interpretable error measure.\n",
    "\n",
    "$\\[\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|\n",
    "\\]$\n",
    "\n",
    "- **R-squared ($\\( R^2 \\)$)**: Proportion of variance explained by the model. Values range from 0 to 1, with higher values indicating a better fit.\n",
    "\n",
    "$\\[\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "\\]$\n",
    "\n",
    "Where $\\( y_i \\)$ is the actual value, $\\( \\hat{y_i} \\)$ is the predicted value, and $\\( \\bar{y} \\)$ is the mean of actual values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af2c00",
   "metadata": {},
   "source": [
    "## 2. Neural Networks and Deep Learning\n",
    "\n",
    "### 2.1 Neural Network Architectures\n",
    "\n",
    "A neural network is a computational model inspired by the structure and function of the brain. It consists of multiple layers of interconnected nodes (neurons) that transform input data through a series of mathematical operations to produce predictions or classifications.\n",
    "\n",
    "**Components of a Neural Network:**\n",
    "1. **Input Layer**: \n",
    "   - The first layer of the network that takes in the input features (e.g., pixel values of an image, sensor readings, etc.).\n",
    "   - Each neuron in this layer represents one feature of the input data.\n",
    "\n",
    "2. **Hidden Layers**: \n",
    "   - One or more layers between the input and output layers.\n",
    "   - Each hidden layer is composed of neurons that apply linear transformations (e.g., matrix multiplication) followed by non-linear activation functions.\n",
    "   - The more hidden layers (depth) and neurons (width) in a network, the more complex relationships it can model.\n",
    "\n",
    "3. **Output Layer**: \n",
    "   - The final layer of the network that outputs predictions or classifications.\n",
    "   - For regression tasks, the output might be a single neuron representing a continuous value.\n",
    "   - For classification tasks, the output might contain multiple neurons with probabilities representing different classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. Different activation functions are used depending on the problem and network architecture.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: \n",
    "  $$ f(x) = \\max(0, x) $$\n",
    "  - ReLU is widely used in hidden layers because it is computationally efficient and helps mitigate the vanishing gradient problem.\n",
    "  - It outputs zero for negative inputs and is linear for positive inputs.\n",
    "\n",
    "- **Sigmoid**: \n",
    "  $$ f(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "  - Maps the input to a range between 0 and 1, making it useful for binary classification problems.\n",
    "  - However, it can suffer from vanishing gradients, making training deep networks difficult.\n",
    "\n",
    "- **Tanh (Hyperbolic Tangent)**: \n",
    "  $$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "  - Maps the input to a range between -1 and 1.\n",
    "  - Centered at zero, making it suitable for hidden layers to normalize the outputs.\n",
    "\n",
    "- **Softmax**: \n",
    "  $$ f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $$\n",
    "  - Used in the output layer for multi-class classification tasks to convert raw scores (logits) into probabilities that sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Loss Functions\n",
    "\n",
    "Loss functions quantify the difference between the predicted output and the actual target values. They guide the optimization process by providing a measure to minimize.\n",
    "\n",
    "- **Cross-Entropy Loss**: \n",
    "  - Used for classification tasks, especially for multi-class classification problems.\n",
    "  - Formula for binary classification:\n",
    "  $$ \\text{Cross-Entropy} = -[y \\cdot \\log(p) + (1-y) \\cdot \\log(1-p)] $$\n",
    "  - Formula for multi-class classification:\n",
    "  $$ \\text{Cross-Entropy} = -\\sum_{i=1}^{C} y_i \\cdot \\log(p_i) $$\n",
    "  where $ y $ is the true label, $ p $ is the predicted probability, and $ C $ is the number of classes.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: \n",
    "  - Commonly used for regression tasks.\n",
    "  - Measures the average squared difference between the actual and predicted values.\n",
    "  $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y_i} $ is the predicted value, and $ n $ is the number of observations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Optimizers\n",
    "\n",
    "Optimizers are algorithms that adjust the model parameters (weights and biases) during training to minimize the loss function.\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent)**: \n",
    "  - Updates model parameters using the gradient of the loss function with respect to the parameters.\n",
    "  - Uses a small batch of data (often a single sample) for each update, making it faster but more noisy compared to batch gradient descent.\n",
    "  - Formula:\n",
    "  $$ \\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta) $$\n",
    "  where $ \\theta $ are the parameters, $ \\eta $ is the learning rate, and $ \\nabla_\\theta J(\\theta) $ is the gradient of the loss function $ J $.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation)**:\n",
    "  - Combines the advantages of both AdaGrad and RMSProp optimizers.\n",
    "  - Maintains an exponentially decaying average of past squared gradients and past gradients, allowing it to adapt the learning rate for each parameter.\n",
    "  - Formula:\n",
    "  $$ m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot \\nabla_\\theta J(\\theta) $$\n",
    "  $$ v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot (\\nabla_\\theta J(\\theta))^2 $$\n",
    "  $$ \\theta = \\theta - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon} $$\n",
    "  where $ m_t $ and $ v_t $ are the first and second moment estimates, $ \\beta_1 $ and $ \\beta_2 $ are exponential decay rates, and $ \\epsilon $ is a small constant to prevent division by zero.\n",
    "\n",
    "- **Other Optimizers**:\n",
    "  - **RMSProp**: Adapts the learning rate based on a moving average of squared gradients.\n",
    "  - **AdaGrad**: Adjusts learning rate based on the sum of all previous squared gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c3cd9e36975f9",
   "metadata": {},
   "source": [
    "# 3. TensorFlow Setup and Basic Usage\n",
    "\n",
    "# Install TensorFlow (if needed)\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Define a simple sequential model in TensorFlow\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),  # Example input shape for MNIST dataset\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()  # Display the model's architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29682473",
   "metadata": {},
   "source": [
    "## 4. Practical Examples and Coding Challenges\n",
    "\n",
    "### Example 1: Classification with TensorFlow (MNIST Dataset)\n",
    "We'll build a simple neural network to classify images from the MNIST dataset, which consists of handwritten digits.\n",
    "\n",
    "### Example 2: Regression Analysis with TensorFlow\n",
    "Implement a neural network to predict housing prices based on features such as number of rooms, square footage, etc.\n",
    "\n",
    "### Example 3: Introduction to Reinforcement Learning (Optional)\n",
    "Cover basic reinforcement learning concepts, such as the Q-learning algorithm, and implement a simple agent using TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6fbdefbc0b60319f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.646501Z",
     "start_time": "2024-10-09T14:35:41.947362Z"
    }
   },
   "source": [
    "# Example 1: Classification with TensorFlow (MNIST)\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Mahra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "C:\\Users\\Mahra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'typeDict'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_103348/1998110050.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# Import required libraries\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmnist\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSequential\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtyping\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_typing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;31m# from tensorflow.python import keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 45\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfeature_column_lib\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfeature_column\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     46\u001B[0m \u001B[1;31m# from tensorflow.python.layers import layers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodule\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;31m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column_v2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msequence_feature_column\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msparse_tensor\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msparse_tensor_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensor_shape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 143\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    144\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcheck_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\layers\\base.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;31m# =============================================================================\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;34m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegacy_tf_layers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[0mInputSpec\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mInputSpec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;31m# See b/110718070#comment18 for more details about this import.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\keras\\models.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmetrics\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mmetrics_module\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptimizer_v1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunctional\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msequential\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0minput_spec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnode\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnode_module\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtraining\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtraining_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtraining_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaving\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaved_model\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnetwork_serialization\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmixed_precision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mloss_scale_optimizer\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mlso\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmixed_precision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaving\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mhdf5_format\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     53\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaving\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msave\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaving\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msaving_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[1;31m# pylint: disable=g-import-not-at-top\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m   \u001B[1;32mimport\u001B[0m \u001B[0mh5py\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m   \u001B[0mHDF5_OBJECT_HEADER_LIMIT\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m64512\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\h5py\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[0m_errors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msilence_errors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 46\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_conv\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mregister_converters\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_register_converters\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     47\u001B[0m \u001B[0m_register_converters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh5py\\h5t.pxd\u001B[0m in \u001B[0;36minit h5py._conv\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mh5py\\h5t.pyx\u001B[0m in \u001B[0;36minit h5py.h5t\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\__init__.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(attr)\u001B[0m\n\u001B[0;32m    318\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mTester\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 320\u001B[1;33m         raise AttributeError(\"module {!r} has no attribute \"\n\u001B[0m\u001B[0;32m    321\u001B[0m                              \"{!r}\".format(__name__, attr))\n\u001B[0;32m    322\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'typeDict'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f0219b3ae1696bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.649501Z",
     "start_time": "2024-10-09T14:35:44.648502Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0e38d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.650502Z",
     "start_time": "2024-10-09T14:35:44.650502Z"
    }
   },
   "source": [
    "# Supervised Learning Example: Linear Regression with scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some random data for demonstration\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X, y, color=\"blue\", label=\"Data Points\")\n",
    "plt.plot(X_test, y_pred, color=\"red\", label=\"Linear Fit\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Linear Regression Example\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e12447d",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Example: K-Means Clustering with scikit-learn\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data for clustering\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='x')\n",
    "plt.title(\"K-Means Clustering Example\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be77af42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.653502Z",
     "start_time": "2024-10-09T14:35:44.652502Z"
    }
   },
   "source": [
    "# Neural Network Example: Simple Feedforward Network using TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(1000, 20)\n",
    "y_train = (np.sum(X_train, axis=1) > 10).astype(int)\n",
    "\n",
    "# Create a simple feedforward neural network model\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(20,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f\"Training Accuracy: {accuracy}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d15cd2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.654503Z",
     "start_time": "2024-10-09T14:35:44.654503Z"
    }
   },
   "source": [
    "# Transformer Example: Using Hugging Face Transformers Library for Text Classification\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Prepare some sample data\n",
    "texts = [\"I love programming.\", \"I dislike bugs.\"]\n",
    "labels = [1, 0]  # 1: Positive, 0: Negative\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=32, return_tensors='tf')\n",
    "input_ids, attention_mask = encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "# Compile and train the BERT model\n",
    "model.compile(optimizer=Adam(learning_rate=3e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit([input_ids, attention_mask], labels, epochs=2)\n",
    "\n",
    "# Evaluate on sample data\n",
    "outputs = model([input_ids, attention_mask])\n",
    "print(\"Model outputs:\", outputs)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2765037f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.655503Z",
     "start_time": "2024-10-09T14:35:44.655503Z"
    }
   },
   "source": [
    "# Logistic Regression Example with scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the iris dataset\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Binary classification: only using class 0 and 1\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6f7f3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.656503Z",
     "start_time": "2024-10-09T14:35:44.656503Z"
    }
   },
   "source": [
    "# Support Vector Machine (SVM) Example with scikit-learn\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification: Only using class 0 and 1\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Support Vector Machine model\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31fc52ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.657502Z",
     "start_time": "2024-10-09T14:35:44.657502Z"
    }
   },
   "source": [
    "# Decision Tree Example with scikit-learn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "434861f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.659503Z",
     "start_time": "2024-10-09T14:35:44.658503Z"
    }
   },
   "source": [
    "# Random Forest Example with scikit-learn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "221ec8b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.660504Z",
     "start_time": "2024-10-09T14:35:44.660504Z"
    }
   },
   "source": [
    "# Hierarchical Clustering Example with scipy and matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=3, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(X, 'ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3efef30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.661504Z",
     "start_time": "2024-10-09T14:35:44.661504Z"
    }
   },
   "source": [
    "# Principal Component Analysis (PCA) Example with scikit-learn\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the first two principal components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.target, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title(\"PCA of Digits Dataset\")\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f205f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.662504Z",
     "start_time": "2024-10-09T14:35:44.662504Z"
    }
   },
   "source": [
    "# t-Distributed Stochastic Neighbor Embedding (t-SNE) Example with scikit-learn\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform t-SNE on the digits dataset\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=digits.target, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title(\"t-SNE of Digits Dataset\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7957b2fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:35:44.663504Z",
     "start_time": "2024-10-09T14:35:44.663504Z"
    }
   },
   "source": [
    "# Association Rule Mining Example: Apriori Algorithm with mlxtend\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Create a small dataset for demonstration\n",
    "data = {'Item1': [1, 0, 1, 1, 0],\n",
    "        'Item2': [0, 1, 0, 1, 0],\n",
    "        'Item3': [1, 1, 1, 0, 1],\n",
    "        'Item4': [0, 0, 0, 1, 0]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply the Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
