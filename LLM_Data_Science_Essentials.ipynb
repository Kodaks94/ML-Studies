{
 "cells": [
  {
   "metadata": {},
   "id": "a4681ef0",
   "cell_type": "markdown",
   "source": "# Essential Knowledge for LLMs, Data Science, and Non-Reinforcement Learning ML\n\nThis notebook covers fundamental concepts and implementations for Large Language Models (LLMs), Data Science, and non-Reinforcement Learning Machine Learning techniques. It is designed to provide a solid foundation for roles in data science, NLP, and general machine learning development.\n\n## Table of Contents\n1. **Large Language Models (LLMs)**\n    - Overview of LLMs and Transformer Architectures\n    - Tokenization and Embeddings\n    - Building and Fine-Tuning with the `transformers` library\n    - Text Generation and Sentiment Analysis\n2. **Data Science Essentials**\n    - Data Preprocessing and Feature Engineering\n    - Exploratory Data Analysis (EDA)\n    - Supervised and Unsupervised Learning Techniques\n3. **Non-Reinforcement Learning Machine Learning**\n    - Time Series Analysis and Forecasting\n    - Anomaly Detection\n    - Transfer Learning with Pre-trained Models\n"
  },
  {
   "metadata": {},
   "id": "0e28a603",
   "cell_type": "markdown",
   "source": "## 1. Large Language Models (LLMs)\n\n### 1.1 Overview of LLMs and Transformer Architectures\nLarge Language Models (LLMs) are deep learning models trained on massive text datasets. They use Transformer architectures, which leverage attention mechanisms to capture relationships between words in a sequence.\n\n**Examples**:\n- **GPT (Generative Pre-trained Transformer)**: A model that can generate text, answer questions, and perform various NLP tasks.\n- **BERT (Bidirectional Encoder Representations from Transformers)**: Designed for understanding the context of words in a sentence.\n\n**Key Components of Transformers**:\n- **Self-Attention Mechanism**: Determines which words in a sentence are important with respect to each other.\n- **Positional Encoding**: Adds information about the position of words in a sentence.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "b492fe62",
   "cell_type": "code",
   "source": "# 1.2 Tokenization and Embeddings\n\n# Using the Hugging Face Transformers library to tokenize text and create embeddings\n\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Load pre-trained BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Sample sentence\nsentence = \"Machine learning is fascinating!\"\n\n# Tokenize the sentence\ninputs = tokenizer(sentence, return_tensors='pt')\n\n# Get embeddings from BERT\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# The last hidden state of the model (embeddings for each token)\nembeddings = outputs.last_hidden_state\nprint(f\"Shape of embeddings: {embeddings.shape}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "id": "62e58e41",
   "cell_type": "markdown",
   "source": "### 1.3 Building and Fine-Tuning with the `transformers` library\nWe can fine-tune a pre-trained model like BERT or GPT-2 for specific tasks such as text classification or text generation.\n\n**Example**: Fine-tuning BERT for sentiment analysis.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "9ea7220d",
   "cell_type": "code",
   "source": "# Example code for fine-tuning a pre-trained BERT model for sentiment analysis\n\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load a sample dataset for sentiment analysis\ndata = {'text': [\"I love machine learning!\", \"This is a boring task.\", \"Deep learning is amazing.\"],\n        'label': [1, 0, 1]}\ndf = pd.DataFrame(data)\n\n# Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['label'], test_size=0.2)\n\n# Tokenize the text data\ntrain_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, return_tensors='pt')\nval_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, return_tensors='pt')\n\n# Create PyTorch datasets\nclass SentimentDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = SentimentDataset(train_encodings, train_labels)\nval_dataset = SentimentDataset(val_encodings, val_labels)\n\n# Load pre-trained BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    warmup_steps=10,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train and evaluate\ntrainer.train()\ntrainer.evaluate()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "id": "4077be01",
   "cell_type": "markdown",
   "source": "## 2. Data Science Essentials\n\n### 2.1 Data Preprocessing and Feature Engineering\nData preprocessing is a critical step in data science. It includes handling missing values, scaling numerical features, and encoding categorical variables.\n\n**Example**: Using `pandas` and `scikit-learn` for basic data preprocessing tasks.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "fe89bde4",
   "cell_type": "code",
   "source": "# Basic Data Preprocessing with Pandas and Scikit-learn\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Create a sample dataframe\ndata = {'age': [25, 32, 47, 51, 23, np.nan],\n        'salary': [50000, 54000, 58000, 60000, 52000, 59000],\n        'department': ['HR', 'Engineering', 'Engineering', 'HR', 'HR', 'Engineering']}\n\ndf = pd.DataFrame(data)\n\n# Handle missing values\ndf['age'].fillna(df['age'].mean(), inplace=True)\n\n# One-hot encode categorical variables\nencoder = OneHotEncoder(sparse=False)\ndepartment_encoded = encoder.fit_transform(df[['department']])\n\n# Standardize numerical features\nscaler = StandardScaler()\ndf[['age', 'salary']] = scaler.fit_transform(df[['age', 'salary']])\n\n# Concatenate the encoded and scaled features\nencoded_df = pd.DataFrame(department_encoded, columns=encoder.get_feature_names_out(['department']))\ndf.reset_index(drop=True, inplace=True)\nfinal_df = pd.concat([df, encoded_df], axis=1).drop(['department'], axis=1)\n\nprint(final_df)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "id": "60942b27",
   "cell_type": "markdown",
   "source": "## 3. Non-Reinforcement Learning Machine Learning\n\n### 3.1 Time Series Analysis and Forecasting\nTime series analysis involves techniques for analyzing time-ordered data points to forecast future values.\n\n**Example**: Building a Long Short-Term Memory (LSTM) network for time series forecasting using TensorFlow/Keras.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "5a1b657e",
   "cell_type": "code",
   "source": "# Time Series Forecasting with LSTM using TensorFlow/Keras\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a synthetic time series dataset\ndef create_dataset(n_points=1000):\n    time = np.arange(0, n_points)\n    data = np.sin(0.1 * time) + np.random.normal(0, 0.1, size=n_points)  # Sine wave with noise\n    return data\n\n# Create dataset\ndata = create_dataset()\nn_timesteps = 50\n\n# Prepare the dataset for LSTM\nX, y = [], []\nfor i in range(len(data) - n_timesteps):\n    X.append(data[i:i+n_timesteps])\n    y.append(data[i+n_timesteps])\nX, y = np.array(X), np.array(y)\n\n# Split into train and test sets\ntrain_size = int(len(X) * 0.8)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# Build LSTM model\nmodel = Sequential([\n    LSTM(50, activation='relu', input_shape=(n_timesteps, 1)),\n    Dense(1)\n])\nmodel.compile(optimizer='adam', loss='mse')\n\n# Reshape input data for LSTM\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate and predict\nloss = model.evaluate(X_test, y_test)\nprint(f'Test Loss: {loss}')\ny_pred = model.predict(X_test)\n\n# Visualize results\nplt.figure(figsize=(10, 6))\nplt.plot(range(len(y_test)), y_test, label='Actual')\nplt.plot(range(len(y_pred)), y_pred, label='Predicted')\nplt.title('LSTM Time Series Forecasting')\nplt.xlabel('Time Steps')\nplt.ylabel('Value')\nplt.legend()\nplt.show()\n",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}