{
 "cells": [
  {
   "metadata": {},
   "id": "da8d1cbf",
   "cell_type": "markdown",
   "source": "# Understanding Transformers in Deep Learning\n\nThis notebook provides a comprehensive guide to Transformers, a state-of-the-art architecture used in natural language processing (NLP) and other deep learning applications. The notebook covers the theoretical concepts, practical implementations, and best practices for using Transformers.\n\n## Table of Contents\n1. **Introduction to Transformers**\n    - Overview and History\n    - Key Components of the Transformer Architecture\n2. **Attention Mechanism**\n    - Self-Attention and Multi-Head Attention\n    - Positional Encoding\n3. **Building Transformers from Scratch**\n    - Implementing a Basic Transformer Model\n    - Understanding the Model Architecture\n4. **Using Pre-trained Transformer Models**\n    - Leveraging `transformers` library (Hugging Face)\n    - Text Classification and Text Generation Examples\n5. **Advanced Topics**\n    - Fine-Tuning Transformers\n    - Handling Long Sequences with Transformers\n"
  },
  {
   "metadata": {},
   "id": "362d3ef9",
   "cell_type": "markdown",
   "source": "## 1. Introduction to Transformers\n\n### 1.1 Overview and History\nThe Transformer architecture was introduced in the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017. It revolutionized NLP by replacing recurrent and convolutional networks with a structure based solely on attention mechanisms. Transformers enable efficient parallelization and have become the backbone for models such as BERT, GPT, and T5.\n\n### 1.2 Key Components of the Transformer Architecture\n1. **Encoder and Decoder**: The Transformer consists of an encoder and a decoder. The encoder processes input sequences, while the decoder generates output sequences.\n2. **Attention Mechanisms**: Self-attention allows the model to weigh the importance of different words in a sequence.\n3. **Positional Encoding**: Adds information about the order of words in a sequence, as Transformers lack inherent sequential information.\n\nIn the following sections, we will explore each of these components in detail and see how they work together to form the Transformer architecture.\n"
  },
  {
   "metadata": {},
   "id": "12946cdc",
   "cell_type": "markdown",
   "source": "## 2. Attention Mechanism\n\n### 2.1 Self-Attention and Multi-Head Attention\nThe core innovation of the Transformer is the self-attention mechanism, which allows the model to look at other words in the same sequence to predict a word. This helps in capturing long-range dependencies more effectively.\n\n#### Self-Attention Calculation\nGiven an input sequence, self-attention is computed as follows:\n\n1. Calculate three vectors: Query (Q), Key (K), and Value (V) for each word in the sequence.\n2. Compute the dot product of Q and K, and scale by the square root of the dimension of K.\n3. Apply a softmax function to get attention scores.\n4. Multiply the attention scores by V to get the output representation.\n\n#### Multi-Head Attention\nInstead of computing a single attention score, multi-head attention uses multiple sets of Q, K, and V, which allows the model to focus on different parts of the sequence simultaneously.\n\n### Implementation of Self-Attention Mechanism\nBelow is a code implementation of a self-attention mechanism in PyTorch.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "837fcf64",
   "cell_type": "code",
   "source": "# Self-Attention Mechanism Implementation\n\nimport torch\nimport torch.nn as nn\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(SelfAttention, self).__init__()\n        self.embed_size = embed_size\n        self.heads = heads\n        self.head_dim = embed_size // heads\n\n        assert (\n            self.head_dim * heads == embed_size\n        ), \"Embedding size needs to be divisible by heads\"\n\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n\n    def forward(self, values, keys, query, mask):\n        N = query.shape[0]\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n\n        # Split the embedding into self.heads different pieces\n        values = values.reshape(N, value_len, self.heads, self.head_dim)\n        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n\n        # Calculate the dot product attention\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # Queries shape: (N, query_len, heads, head_dim)\n                                                                   # Keys shape: (N, key_len, heads, heads_dim)\n                                                                   # Energy shape: (N, heads, query_len, key_len)\n\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n\n        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)  # Normalize across key_len\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads * self.head_dim)\n        out = self.fc_out(out)\n\n        return out\n\n# Test the self-attention mechanism\nembed_size = 256\nheads = 8\nattention = SelfAttention(embed_size, heads)\n\n# Create sample input\nx = torch.rand(64, 10, embed_size)  # (batch_size, sequence_length, embed_size)\nmask = None\noutput = attention(x, x, x, mask)\n\nprint(f\"Output shape: {output.shape}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "id": "4d7585da",
   "cell_type": "markdown",
   "source": "## 3. Building Transformers from Scratch\n\nLet's implement a simplified version of a Transformer architecture, including both the encoder and decoder components. We'll use PyTorch for the implementation.\n\n**Note**: This implementation is for educational purposes and does not include advanced optimizations used in actual Transformer models like GPT or BERT.\n"
  },
  {
   "metadata": {},
   "id": "acab3f64",
   "cell_type": "markdown",
   "source": "## 4. Using Pre-trained Transformer Models\n\nThe `transformers` library by Hugging Face provides easy access to numerous pre-trained models. In this section, we will see how to leverage these models for downstream NLP tasks.\n\n### 4.1 Text Classification\nWe'll use BERT to classify text into different categories.\n\n### 4.2 Text Generation\nWe'll use GPT-2 for generating text based on a given prompt.\n\nBelow is the implementation for both examples:\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "id": "4b7cc006",
   "cell_type": "code",
   "source": "# Text Classification using Pre-trained BERT\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\n\n# Define a sample dataset\nclass SampleDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        encoding = self.tokenizer.encode_plus(\n            self.texts[item],\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': self.texts[item],\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(self.labels[item], dtype=torch.long)\n        }\n\n# Sample data\ntexts = [\"I love programming\", \"Transformers are powerful models\", \"Machine learning is fascinating\"]\nlabels = [1, 0, 1]  # Assume 1 for positive sentiment, 0 for neutral/negative\n\n# Load pre-trained tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# Create DataLoader\ndataset = SampleDataset(texts, labels, tokenizer, max_len=10)\nloader = DataLoader(dataset, batch_size=2)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = nn.CrossEntropyLoss()\n\n# Training loop (simplified)\nfor batch in loader:\n    input_ids = batch['input_ids']\n    attention_mask = batch['attention_mask']\n    labels = batch['labels']\n\n    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n    loss = outputs.loss\n    logits = outputs.logits\n\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\nprint(f\"Training completed. Final loss: {loss.item():.4f}\")\n",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}