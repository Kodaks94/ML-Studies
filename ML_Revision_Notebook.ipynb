{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Revision Notebook",
   "id": "729c6c5ad105da2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Basic Machine Learning Concepts\n",
    "\n",
    "### 1.1 Supervised vs. Unsupervised Learning\n",
    "\n",
    "#### Supervised Learning\n",
    "Supervised learning refers to a class of algorithms that learn from *labeled data*, where each input is paired with an output label. The goal is to learn a function that maps inputs to outputs based on example input-output pairs. The algorithm tries to minimize the error between its predictions and the actual labels.\n",
    "\n",
    "- **Examples**:\n",
    "  - **Regression**: Predicting continuous values (e.g., house prices, stock prices).\n",
    "  - **Classification**: Predicting discrete values (e.g., spam detection, image classification).\n",
    "\n",
    "- **Key characteristics**:\n",
    "  - Requires labeled data.\n",
    "  - Has clear goals defined by the labels.\n",
    "  - Can be used for both classification and regression tasks.\n",
    "\n",
    "- **Common Algorithms**:\n",
    "  - Linear Regression\n",
    "  - Logistic Regression\n",
    "  - Support Vector Machines (SVM)\n",
    "  - Decision Trees\n",
    "  - Neural Networks\n",
    "\n",
    "#### Unsupervised Learning\n",
    "Unsupervised learning deals with *unlabeled data*. The goal is to identify hidden patterns or intrinsic structures within the data without predefined labels.\n",
    "\n",
    "- **Examples**:\n",
    "  - **Clustering**: Grouping similar items together (e.g., customer segmentation).\n",
    "  - **Association Rule Learning**: Discovering relationships between variables in large datasets (e.g., market basket analysis).\n",
    "\n",
    "- **Key characteristics**:\n",
    "  - No labeled output.\n",
    "  - Learns the underlying structure from the data itself.\n",
    "  - Often used for exploratory data analysis.\n",
    "\n",
    "- **Common Algorithms**:\n",
    "  - K-Means Clustering\n",
    "  - Hierarchical Clustering\n",
    "  - Principal Component Analysis (PCA)\n",
    "  - t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "  - Association Rule Mining (e.g., Apriori algorithm)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Common Machine Learning Algorithms\n",
    "\n",
    "#### Linear Regression\n",
    "Linear regression is a simple, interpretable model used for predicting continuous outcomes based on linear relationships between the input features and the target variable. The model assumes a linear dependency, represented as:\n",
    "\n",
    "$$\\[\n",
    "y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon\n",
    "\\]$$\n",
    "\n",
    "where:\n",
    "- $$\\( y \\)$$ is the target variable.\n",
    "- $\\( x_1, x_2, \\ldots, x_n \\)$ are input features.\n",
    "- $\\( \\beta_0 \\)$ is the intercept.\n",
    "- $\\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\)$ are the coefficients.\n",
    "- $\\( \\epsilon \\)$ is the error term.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple and interpretable.\n",
    "  - Computationally efficient.\n",
    "  - Works well when there is a linear relationship between input features and the target.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Poor performance when there are complex nonlinear relationships.\n",
    "  - Sensitive to outliers.\n",
    "\n",
    "#### Logistic Regression\n",
    "Logistic regression is used for binary classification tasks. It estimates the probability of a binary outcome by using the logistic (sigmoid) function to map predicted values to probabilities:\n",
    "\n",
    "$\\[\n",
    "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n)}}\n",
    "\\]$\n",
    "\n",
    "- **Advantages**:\n",
    "  - Good for binary classification with linear boundaries.\n",
    "  - Can handle unbalanced datasets using techniques like class weights.\n",
    "  \n",
    "- **Limitations**:\n",
    "  - Not suitable for complex relationships.\n",
    "  - Can overfit with high-dimensional data if regularization is not used.\n",
    "\n",
    "#### Decision Trees and Random Forests\n",
    "- **Decision Trees**:\n",
    "  - A tree-based model that splits the data recursively based on feature values, creating branches that lead to decisions or predictions at the leaves.\n",
    "  - The splits are based on criteria like Information Gain, Gini Impurity, or Chi-Square statistics.\n",
    "\n",
    "- **Random Forests**:\n",
    "  - An ensemble method that builds multiple decision trees and combines their results to improve prediction accuracy and control overfitting.\n",
    "  - It reduces variance by averaging multiple trees, each trained on a different subset of the data.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Can handle both numerical and categorical features.\n",
    "  - Less preprocessing required (e.g., no need for feature scaling).\n",
    "\n",
    "- **Limitations**:\n",
    "  - Can be prone to overfitting (especially decision trees).\n",
    "  - High computational cost for large forests.\n",
    "\n",
    "#### K-Means Clustering\n",
    "K-means clustering is an unsupervised algorithm that partitions data into $\\( K \\)$ clusters based on feature similarity. The algorithm works by:\n",
    "\n",
    "1. Initializing $\\( K \\)$ cluster centroids.\n",
    "2. Assigning each data point to the nearest centroid.\n",
    "3. Updating centroids by calculating the mean of assigned points.\n",
    "4. Repeating the process until convergence.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple and easy to implement.\n",
    "  - Scales well with a large number of samples.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Requires pre-specifying the number of clusters $\\( K \\)$.\n",
    "  - Sensitive to outliers and initial centroid selection.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Evaluation Metrics\n",
    "\n",
    "#### Classification Metrics\n",
    "Evaluating classification models involves assessing how well they distinguish between classes. Common metrics include:\n",
    "\n",
    "- **Accuracy**: Proportion of correctly predicted labels out of the total predictions.\n",
    "\n",
    "$\\[\n",
    "\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Samples}}\n",
    "\\]$\n",
    "\n",
    "- **Precision**: Proportion of correctly predicted positive observations out of all predicted positives.\n",
    "\n",
    "$\\[\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "\\]$\n",
    "\n",
    "- **Recall**: Proportion of correctly predicted positive observations out of all actual positives.\n",
    "\n",
    "$\\[\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "\\]$\n",
    "\n",
    "- **F1-Score**: Harmonic mean of precision and recall. It balances the two when dealing with unbalanced datasets.\n",
    "\n",
    "$\\[\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]$\n",
    "\n",
    "- **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**: Measures the trade-off between true positive rate and false positive rate across different threshold values.\n",
    "\n",
    "#### Regression Metrics\n",
    "Regression models predict continuous values, so their performance is evaluated using metrics that assess prediction error:\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Average squared difference between actual and predicted values. Sensitive to outliers.\n",
    "\n",
    "$\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "\\]$\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: Average absolute difference between actual and predicted values. Provides a more interpretable error measure.\n",
    "\n",
    "$\\[\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|\n",
    "\\]$\n",
    "\n",
    "- **R-squared ($\\( R^2 \\)$)**: Proportion of variance explained by the model. Values range from 0 to 1, with higher values indicating a better fit.\n",
    "\n",
    "$\\[\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "\\]$\n",
    "\n",
    "Where $\\( y_i \\)$ is the actual value, $\\( \\hat{y_i} \\)$ is the predicted value, and $\\( \\bar{y} \\)$ is the mean of actual values.\n"
   ],
   "id": "19d59d4e68212f65"
  },
  {
   "cell_type": "markdown",
   "id": "76af2c00",
   "metadata": {},
   "source": [
    "## 2. Neural Networks and Deep Learning\n",
    "\n",
    "### 2.1 Neural Network Architectures\n",
    "\n",
    "A neural network is a computational model inspired by the structure and function of the brain. It consists of multiple layers of interconnected nodes (neurons) that transform input data through a series of mathematical operations to produce predictions or classifications.\n",
    "\n",
    "**Components of a Neural Network:**\n",
    "1. **Input Layer**: \n",
    "   - The first layer of the network that takes in the input features (e.g., pixel values of an image, sensor readings, etc.).\n",
    "   - Each neuron in this layer represents one feature of the input data.\n",
    "\n",
    "2. **Hidden Layers**: \n",
    "   - One or more layers between the input and output layers.\n",
    "   - Each hidden layer is composed of neurons that apply linear transformations (e.g., matrix multiplication) followed by non-linear activation functions.\n",
    "   - The more hidden layers (depth) and neurons (width) in a network, the more complex relationships it can model.\n",
    "\n",
    "3. **Output Layer**: \n",
    "   - The final layer of the network that outputs predictions or classifications.\n",
    "   - For regression tasks, the output might be a single neuron representing a continuous value.\n",
    "   - For classification tasks, the output might contain multiple neurons with probabilities representing different classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. Different activation functions are used depending on the problem and network architecture.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: \n",
    "  $$ f(x) = \\max(0, x) $$\n",
    "  - ReLU is widely used in hidden layers because it is computationally efficient and helps mitigate the vanishing gradient problem.\n",
    "  - It outputs zero for negative inputs and is linear for positive inputs.\n",
    "\n",
    "- **Sigmoid**: \n",
    "  $$ f(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "  - Maps the input to a range between 0 and 1, making it useful for binary classification problems.\n",
    "  - However, it can suffer from vanishing gradients, making training deep networks difficult.\n",
    "\n",
    "- **Tanh (Hyperbolic Tangent)**: \n",
    "  $$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "  - Maps the input to a range between -1 and 1.\n",
    "  - Centered at zero, making it suitable for hidden layers to normalize the outputs.\n",
    "\n",
    "- **Softmax**: \n",
    "  $$ f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $$\n",
    "  - Used in the output layer for multi-class classification tasks to convert raw scores (logits) into probabilities that sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Loss Functions\n",
    "\n",
    "Loss functions quantify the difference between the predicted output and the actual target values. They guide the optimization process by providing a measure to minimize.\n",
    "\n",
    "- **Cross-Entropy Loss**: \n",
    "  - Used for classification tasks, especially for multi-class classification problems.\n",
    "  - Formula for binary classification:\n",
    "  $$ \\text{Cross-Entropy} = -[y \\cdot \\log(p) + (1-y) \\cdot \\log(1-p)] $$\n",
    "  - Formula for multi-class classification:\n",
    "  $$ \\text{Cross-Entropy} = -\\sum_{i=1}^{C} y_i \\cdot \\log(p_i) $$\n",
    "  where $ y $ is the true label, $ p $ is the predicted probability, and $ C $ is the number of classes.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: \n",
    "  - Commonly used for regression tasks.\n",
    "  - Measures the average squared difference between the actual and predicted values.\n",
    "  $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y_i} $ is the predicted value, and $ n $ is the number of observations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Optimizers\n",
    "\n",
    "Optimizers are algorithms that adjust the model parameters (weights and biases) during training to minimize the loss function.\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent)**: \n",
    "  - Updates model parameters using the gradient of the loss function with respect to the parameters.\n",
    "  - Uses a small batch of data (often a single sample) for each update, making it faster but more noisy compared to batch gradient descent.\n",
    "  - Formula:\n",
    "  $$ \\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta) $$\n",
    "  where $ \\theta $ are the parameters, $ \\eta $ is the learning rate, and $ \\nabla_\\theta J(\\theta) $ is the gradient of the loss function $ J $.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation)**:\n",
    "  - Combines the advantages of both AdaGrad and RMSProp optimizers.\n",
    "  - Maintains an exponentially decaying average of past squared gradients and past gradients, allowing it to adapt the learning rate for each parameter.\n",
    "  - Formula:\n",
    "  $$ m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot \\nabla_\\theta J(\\theta) $$\n",
    "  $$ v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot (\\nabla_\\theta J(\\theta))^2 $$\n",
    "  $$ \\theta = \\theta - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon} $$\n",
    "  where $ m_t $ and $ v_t $ are the first and second moment estimates, $ \\beta_1 $ and $ \\beta_2 $ are exponential decay rates, and $ \\epsilon $ is a small constant to prevent division by zero.\n",
    "\n",
    "- **Other Optimizers**:\n",
    "  - **RMSProp**: Adapts the learning rate based on a moving average of squared gradients.\n",
    "  - **AdaGrad**: Adjusts learning rate based on the sum of all previous squared gradients.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. TensorFlow Setup and Basic Usage\n",
    "\n",
    "# Install TensorFlow (if needed)\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Define a simple sequential model in TensorFlow\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),  # Example input shape for MNIST dataset\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()  # Display the model's architecture\n"
   ],
   "id": "e53c3cd9e36975f9"
  },
  {
   "cell_type": "markdown",
   "id": "29682473",
   "metadata": {},
   "source": [
    "## 4. Practical Examples and Coding Challenges\n",
    "\n",
    "### Example 1: Classification with TensorFlow (MNIST Dataset)\n",
    "We'll build a simple neural network to classify images from the MNIST dataset, which consists of handwritten digits.\n",
    "\n",
    "### Example 2: Regression Analysis with TensorFlow\n",
    "Implement a neural network to predict housing prices based on features such as number of rooms, square footage, etc.\n",
    "\n",
    "### Example 3: Introduction to Reinforcement Learning (Optional)\n",
    "Cover basic reinforcement learning concepts, such as the Q-learning algorithm, and implement a simple agent using TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6fbdefbc0b60319f",
   "metadata": {},
   "source": [
    "# Example 1: Classification with TensorFlow (MNIST)\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0219b3ae1696bb5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
