{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ab6883",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression is a statistical method to model the relationship between a dependent variable and one or more independent variables. It fits a straight line to the data, known as the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e41678",
   "metadata": {},
   "source": [
    "## 1. Simple Linear Regression\n",
    "The equation of a simple linear regression is:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1 x + \\epsilon \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\): Dependent variable\n",
    "- \\( x \\): Independent variable\n",
    "- \\( \\beta_0 \\): Intercept\n",
    "- \\( \\beta_1 \\): Slope\n",
    "- \\( \\epsilon \\): Error term or residual\n",
    "\n",
    "The objective of linear regression is to minimize the sum of squared errors (SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f54a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some example data for demonstration\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 50)\n",
    "y = 2.5 * x + np.random.normal(0, 5, 50)  # Linear relationship with noise\n",
    "\n",
    "# Calculate the coefficients of the linear regression line using the formula\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "beta0 = y_mean - beta1 * x_mean\n",
    "\n",
    "# Predicted values of y\n",
    "y_pred = beta0 + beta1 * x\n",
    "\n",
    "# Plot the data and the regression line\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, y_pred, color='red', label=f'Regression Line: y = {beta0:.2f} + {beta1:.2f}x')\n",
    "plt.xlabel('Independent Variable (x)')\n",
    "plt.ylabel('Dependent Variable (y)')\n",
    "plt.title('Linear Regression Example')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbf8ca",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. **Data Generation**: We create a linear relationship between \\(x\\) and \\(y\\) with some noise.\n",
    "2. **Coefficient Calculation**: We calculate the slope (\\( \\beta_1 \\)) and intercept (\\( \\beta_0 \\)) using the ordinary least squares method.\n",
    "3. **Plot**: The scatter plot shows the data points, and the red line is the best-fit regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcde47",
   "metadata": {},
   "source": [
    "## 2. Cost Function and Error Minimization\n",
    "The cost function for linear regression is the sum of squared errors (SSE):\n",
    "\n",
    "\\[ \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\): Actual value of the dependent variable\n",
    "- \\( \\hat{y}_i \\): Predicted value from the regression line\n",
    "\n",
    "The goal is to find the values of \\( \\beta_0 \\) and \\( \\beta_1 \\) that minimize the SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function to compute the sum of squared errors (SSE)\n",
    "def cost_function(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "# Calculate the SSE for the current model\n",
    "sse = cost_function(y, y_pred)\n",
    "print(f\"Sum of Squared Errors (SSE): {sse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fe633",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. **Cost Function**: We define a function to compute the SSE.\n",
    "2. **SSE Calculation**: The SSE for the model is calculated and printed."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
